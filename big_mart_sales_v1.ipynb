{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data//formatted_data//test.csv')\n",
    "train = pd.read_csv('data//formatted_data//train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8523, 13) (5681, 12) (14204, 13)\n",
      "\n",
      "Frequency of Categories for varible Item_Fat_Content\n",
      "Low Fat    8485\n",
      "Regular    4824\n",
      "LF          522\n",
      "reg         195\n",
      "low fat     178\n",
      "Name: Item_Fat_Content, dtype: int64\n",
      "\n",
      "Frequency of Categories for varible Item_Type\n",
      "Fruits and Vegetables    2013\n",
      "Snack Foods              1989\n",
      "Household                1548\n",
      "Frozen Foods             1426\n",
      "Dairy                    1136\n",
      "Baking Goods             1086\n",
      "Canned                   1084\n",
      "Health and Hygiene        858\n",
      "Meat                      736\n",
      "Soft Drinks               726\n",
      "Breads                    416\n",
      "Hard Drinks               362\n",
      "Others                    280\n",
      "Starchy Foods             269\n",
      "Breakfast                 186\n",
      "Seafood                    89\n",
      "Name: Item_Type, dtype: int64\n",
      "\n",
      "Frequency of Categories for varible Outlet_Size\n",
      "Medium    4655\n",
      "Small     3980\n",
      "High      1553\n",
      "Name: Outlet_Size, dtype: int64\n",
      "\n",
      "Frequency of Categories for varible Outlet_Location_Type\n",
      "Tier 3    5583\n",
      "Tier 2    4641\n",
      "Tier 1    3980\n",
      "Name: Outlet_Location_Type, dtype: int64\n",
      "\n",
      "Frequency of Categories for varible Outlet_Type\n",
      "Supermarket Type1    9294\n",
      "Grocery Store        1805\n",
      "Supermarket Type3    1559\n",
      "Supermarket Type2    1546\n",
      "Name: Outlet_Type, dtype: int64\n",
      "Original #missing: 2439\n",
      "Final #missing: 0\n",
      "Mode for each Outlet_Type:\n",
      "                  Outlet_Size\n",
      "Outlet_Type                  \n",
      "Grocery Store           Small\n",
      "Supermarket Type1       Small\n",
      "Supermarket Type2      Medium\n",
      "Supermarket Type3      Medium\n",
      "\n",
      "Original #missing: 4016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:245: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final #missing: 0\n",
      "Number of 0 values initially: 879\n",
      "Number of 0 values after modification: 0\n",
      "count    14204.000000\n",
      "mean         1.061884\n",
      "std          0.235907\n",
      "min          0.844563\n",
      "25%          0.925131\n",
      "50%          0.999070\n",
      "75%          1.042007\n",
      "max          3.010094\n",
      "Name: Item_Visibility_MeanRatio, dtype: float64\n",
      "Original Categories:\n",
      "Low Fat    8485\n",
      "Regular    4824\n",
      "LF          522\n",
      "reg         195\n",
      "low fat     178\n",
      "Name: Item_Fat_Content, dtype: int64\n",
      "\n",
      "Modified Categories:\n",
      "Low Fat       6499\n",
      "Regular       5019\n",
      "Non-Edible    2686\n",
      "Name: Item_Fat_Content, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Steps\n",
    "    1. Hypothesis\n",
    "    2. Data Exploration     \n",
    "    3. Data Cleaning\n",
    "    4. Featuring Engineering\n",
    "'''\n",
    "\n",
    "# Data Exploration\n",
    "#   1. Merging Test and Train\n",
    "train['source'] = 'train'\n",
    "test['source'] = 'test'\n",
    "data = pd.concat([train,test], ignore_index=True, sort=False)\n",
    "print(train.shape, test.shape, data.shape)\n",
    "\n",
    "#   2. Exploring null values\n",
    "data.apply(lambda x: sum(x.isnull()))\n",
    "\n",
    "#   3. Exploring overall data stats\n",
    "data.describe()\n",
    "\n",
    "#   4. Exploring unique data in each column (especially useful for categorical columns)\n",
    "data.apply(lambda x: len(x.unique()))\n",
    "#     data.dtypes can help show which columns are categorical\n",
    "\n",
    "#   5. Filter categorical variables (excluding ID cols and source)\n",
    "cat_cols = [x for x in data.dtypes.index if data.dtypes[x]=='object']\n",
    "cat_cols = [x for x in cat_cols if x not in ['Item_Identifier','Outlet_Identifier','source']]\n",
    "\n",
    "#   6. Print frequency of categories\n",
    "for col in cat_cols:\n",
    "    print('\\nFrequency of Categories for varible %s'%col)\n",
    "    print(data[col].value_counts())\n",
    "\n",
    "#   7. Exploration Findings/Notes\n",
    "'''\n",
    "General Description: there are 1559 items sold at 10 different stores\n",
    "1. Null Value Columns: [Item_Weight:2439, Outlet_Size: 4016, Item_Outlet_Sales:5681]\n",
    "2. Stats Description: \n",
    "     a. May want to standardize year column in different format other than 1985 to 2009.\n",
    "     b. Item_Visibility has a min value of zero. This makes no practical sense because when a product is being sold in a store, the visibility cannot be 0.\n",
    "3. Category Issues\n",
    "     a. Item_Fat_Content: ([LF,low fat, Low Fat], [Regular, reg]) *should only be two distinct categories*\n",
    "     b. Outlet_Type: *May need to combine supermarket types 1,2,3*\n",
    "     c. Item_Type: Not all categories have substantial numbers. It looks like combining them can give better results.\n",
    "     \n",
    "'''\n",
    "\n",
    "#Data Cleaning\n",
    "\n",
    "#   8. Null Value Replacement for Item Weight\n",
    "#     a. Determine the average weight per item\n",
    "item_avg_weight = data.pivot_table(values='Item_Weight', index='Item_Identifier')\n",
    "#     b. Get a boolean variable specifying missing Item_Weight values\n",
    "miss_bool = data['Item_Weight'].isnull()\n",
    "#     c. Impute data and check #missing values before and after imputation to confirm\n",
    "print('Original #missing: %d'%sum(miss_bool))\n",
    "data.loc[miss_bool,'Item_Weight'] = data.loc[miss_bool, 'Item_Identifier'].apply(lambda x: item_avg_weight.loc[item_avg_weight.index==x,'Item_Weight'][0])\n",
    "print('Final #missing: %d'% sum(data['Item_Weight'].isnull()))\n",
    "\n",
    "#   9. Null Value Replacement for Outlet size\n",
    "#     a. Determine the mode outlet size per outlet type\n",
    "outlet_size_mode = data[pd.notna(data['Outlet_Size'])==True].pivot_table(values='Outlet_Size', index='Outlet_Type',aggfunc=(lambda x:mode(x).mode[0]))\n",
    "print('Mode for each Outlet_Type:')\n",
    "print(outlet_size_mode)\n",
    "#     b. Get a boolean variable specifying missingOutlet_Size values\n",
    "miss_bool = data['Outlet_Size'].isnull()\n",
    "#     c. Impute data and check #missing values before and after imputation to confirm\n",
    "print('\\nOriginal #missing: %d'%sum(miss_bool))\n",
    "data.loc[miss_bool,'Outlet_Size'] = data.loc[miss_bool, 'Outlet_Type'].apply(lambda x: outlet_size_mode.loc[outlet_size_mode.index==x,'Outlet_Size'][0])\n",
    "print('Final #missing: %d'% sum(data['Item_Weight'].isnull()))\n",
    "\n",
    "\n",
    "#Feature Engineering\n",
    "#   10. Consider Combining Outlet_Type\n",
    "#     a. Exploration (keeping outlet type as is because significant difference between different types)\n",
    "data.pivot_table(values='Item_Outlet_Sales', index='Outlet_Type')\n",
    "\n",
    "#   11. Modify Item Visibility\n",
    "#     a. Determine average visibility of a product\n",
    "visibility_avg = data.pivot_table(values='Item_Visibility', index='Item_Identifier')\n",
    "#     b. Impute 0 values with mean visibility of that product:\n",
    "miss_bool = (data['Item_Visibility']==0)\n",
    "print('Number of 0 values initially: %d'%sum(miss_bool))\n",
    "data.loc[miss_bool,'Item_Visibility'] = data.loc[miss_bool,'Item_Identifier'].apply(lambda x: visibility_avg.loc[visibility_avg.index==x,'Item_Visibility'][0])\n",
    "print('Number of 0 values after modification: %d'%sum(data['Item_Visibility']==0))\n",
    "#     c. Creating modifided Item Visibility value that is compared to the mean visibility across all stores\n",
    "data['Item_Visibility_MeanRatio'] = data.apply(lambda x: x['Item_Visibility']/visibility_avg.loc[visibility_avg.index==x['Item_Identifier'],'Item_Visibility'][0], axis=1)\n",
    "print(data['Item_Visibility_MeanRatio'].describe())\n",
    "\n",
    "#   12. Create New Column for Item Type (!!!! Should Modify to group better !!!!)\n",
    "#     a. Get the first two charcters of ID and rename to more intuitive categories\n",
    "data['Item_Type_Combined'] = data['Item_Identifier'].apply(lambda x: x[0:2])\n",
    "data['Item_Type_Combined'] = data['Item_Type_Combined'].map({'FD':'Food',\n",
    "                                                            'NC':'Non-Consumable',\n",
    "                                                            'DR':'Drinks'})\n",
    "data['Item_Type_Combined'].value_counts()\n",
    "\n",
    "#   13. Creating a new column depicting the years of operation\n",
    "data['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']\n",
    "data['Outlet_Years'].describe()\n",
    "\n",
    "#   14. Modifying Item_Fat Content Categories\n",
    "print('Original Categories:')\n",
    "print(data['Item_Fat_Content'].value_counts())\n",
    "print('\\nModified Categories:')\n",
    "data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat',\n",
    "                                                            'reg':'Regular',\n",
    "                                                            'low fat': 'Low Fat'})\n",
    "#Makes sure that non-consumable items don't have fat content listed\n",
    "data.loc[data['Item_Type_Combined']=='Non-Consumable','Item_Fat_Content'] = 'Non-Edible'\n",
    "print(data['Item_Fat_Content'].value_counts())\n",
    "\n",
    "#   15. One-Hot Encoding Categorical Variables\n",
    "le = preprocessing.LabelEncoder()\n",
    "#     a. New vairable for outlet\n",
    "data['Outlet'] = le.fit_transform(data['Outlet_Identifier'].values)\n",
    "\n",
    "#     b. Label Encoding Remaining Categorical Variables\n",
    "var_mod = ['Item_Fat_Content', 'Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet']\n",
    "for i in var_mod:\n",
    "    data[i] = le.fit_transform(data[i])\n",
    "\n",
    "#     c. One Hot Coding categorical label encoded columns\n",
    "data = pd.get_dummies(data,columns=var_mod)\n",
    "\n",
    "#   16. Convert Data Back into Test and Train sets\n",
    "#     a. Drops the columns that have been encoded to better formats and are no longer needed\n",
    "data.drop(['Item_Type','Outlet_Establishment_Year'],axis=1,inplace=True)\n",
    "#     b. Divide into test and train:\n",
    "train = data.loc[data['source']==\"train\"].copy()\n",
    "train.drop(['source'],axis=1,inplace=True)\n",
    "test = data.loc[data['source']==\"test\"].copy()\n",
    "test.drop(['Item_Outlet_Sales','source'],axis=1,inplace=True)\n",
    "#     c. Export files as modified versions:\n",
    "train.to_csv(\"data/formatted_data/train_modified.csv\",index=False)\n",
    "test.to_csv(\"data/formatted_data/test_modified.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Mean Based\n",
    "mean_sales = train['Item_Outlet_Sales'].mean()\n",
    "\n",
    "#   Define a dataframe with IDs for submission:\n",
    "base1 = test[['Item_Identifier','Outlet_Identifier']]\n",
    "base1['Item_Outlet_Sales'] = mean_sales\n",
    "\n",
    "#   Export submission file\n",
    "base1.to_csv(\"alg0.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
